{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>07 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>[위스콘신 유방암 데이터 세트 생성]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>[정규 분표 형태의 표준 스케일링 적용]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# StandardScaler( )로 평균이 0, 분산 1로 데이터 분포도 변환\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(cancer.data)\n",
    "\n",
    "# 데이터 세트 분리\n",
    "X_train , X_test, y_train , y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>[로지스틱 회귀를 이용해 학습 및 예측 수행]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.977, roc_auc:0.995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 로지스틱 회귀를 이용하여 학습 및 예측 수행. \n",
    "# solver인자값을 생성자로 입력하지 않으면 solver='lbfgs'  \n",
    "lr_clf = LogisticRegression() # solver='lbfgs'\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_preds = lr_clf.predict(X_test)\n",
    "lr_preds_proba = lr_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# accuracy와 roc_auc 측정\n",
    "print('accuracy: {0:.3f}, roc_auc:{1:.3f}'.format(accuracy_score(y_test, lr_preds),\n",
    "                                                 roc_auc_score(y_test , lr_preds_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 출력 결과 solver가 lbfgs일 경우 정확도가 0.977, ROC-AUC가 0.995로 도출됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<서로 다른 solver 값으로 학습 후 성능 평가>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solver:lbfgs, accuracy: 0.977, roc_auc:0.995\n",
      "solver:liblinear, accuracy: 0.982, roc_auc:0.995\n",
      "solver:newton-cg, accuracy: 0.977, roc_auc:0.995\n",
      "solver:sag, accuracy: 0.982, roc_auc:0.995\n",
      "solver:saga, accuracy: 0.982, roc_auc:0.995\n"
     ]
    }
   ],
   "source": [
    "solvers = ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']\n",
    "# 여러개의 solver값 별로 LogisticRegression 학습 후 성능 평가\n",
    "for solver in solvers:\n",
    "    lr_clf = LogisticRegression(solver=solver, max_iter=600) # max_iter는 solver로 지정된 최적화 알고리즘이 최적 수렴할 수 있는 최대 반복 횟수\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    lr_preds = lr_clf.predict(X_test)\n",
    "    lr_preds_proba = lr_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # accuracy와 roc_auc 측정\n",
    "    print('solver:{0}, accuracy: {1:.3f}, roc_auc:{2:.3f}'.format(solver, \n",
    "                                                                  accuracy_score(y_test, lr_preds),\n",
    "                                                                  roc_auc_score(y_test , lr_preds_proba)))                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 출력 결과 solver별 차이는 크지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<solver, penalty, C 최적화>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터:{'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}, 최적 평균 정확도:0.979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s2102\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "15 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\s2102\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\s2102\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\s2102\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\s2102\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\s2102\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.96485659 0.94555834 0.92261209        nan 0.97891024 0.97364708\n",
      " 0.96131997        nan 0.97539218 0.97539218 0.96660169        nan\n",
      " 0.97011974 0.96836536 0.96662025        nan 0.96661097 0.96661097\n",
      " 0.96134781        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params={'solver':['liblinear', 'lbfgs'],\n",
    "        'penalty':['l2', 'l1'],\n",
    "        'C':[0.01, 0.1, 1, 5, 10]}\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3 )\n",
    "grid_clf.fit(data_scaled, cancer.target)\n",
    "print('최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}'.format(grid_clf.best_params_, \n",
    "                                                  grid_clf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 출력 결과 FitFailedWarning 메시지가 함께 나오는데 이는 solver가 lbfgs일 때 L1 규제를 지원하지 않음에도 GridSearchCV에서 L1 규제값을 입력했기 때문."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>08 회귀 트리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.datasets import load_boston\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.ensemble import RandomForestRegressor\\nimport pandas as pd\\nimport numpy as np\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')  #사이킷런 1.2 부터는 보스턴 주택가격 데이터가 없어진다는 warning 메시지 출력 제거\\n\\n# 보스턴 데이터 세트 로드\\nboston = load_boston()\\nbostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)\\n\\nbostonDF[\\'PRICE\\'] = boston.target\\ny_target = bostonDF[\\'PRICE\\']\\nX_data = bostonDF.drop([\\'PRICE\\'], axis=1,inplace=False)\\n\\nrf = RandomForestRegressor(random_state=0, n_estimators=1000)\\nneg_mse_scores = cross_val_score(rf, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\\nrmse_scores  = np.sqrt(-1 * neg_mse_scores)\\navg_rmse = np.mean(rmse_scores)\\n\\nprint(\\' 5 교차 검증의 개별 Negative MSE scores: \\', np.round(neg_mse_scores, 2))\\nprint(\\' 5 교차 검증의 개별 RMSE scores : \\', np.round(rmse_scores, 2))\\nprint(\\' 5 교차 검증의 평균 RMSE : {0:.3f} \\'.format(avg_rmse))'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  #사이킷런 1.2 부터는 보스턴 주택가격 데이터가 없어진다는 warning 메시지 출력 제거\n",
    "\n",
    "# 보스턴 데이터 세트 로드\n",
    "boston = load_boston()\n",
    "bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "\n",
    "bostonDF['PRICE'] = boston.target\n",
    "y_target = bostonDF['PRICE']\n",
    "X_data = bostonDF.drop(['PRICE'], axis=1,inplace=False)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=0, n_estimators=1000)\n",
    "neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "rmse_scores  = np.sqrt(-1 * neg_mse_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "print(' 5 교차 검증의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2))\n",
    "print(' 5 교차 검증의 개별 RMSE scores : ', np.round(rmse_scores, 2))\n",
    "print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_model_cv_prediction(model, X_data, y_target):\\n    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\\n    rmse_scores  = np.sqrt(-1 * neg_mse_scores)\\n    avg_rmse = np.mean(rmse_scores)\\n    print(\\'##### \\',model.__class__.__name__ , \\' #####\\')\\n    print(\\' 5 교차 검증의 평균 RMSE : {0:.3f} \\'.format(avg_rmse))'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def get_model_cv_prediction(model, X_data, y_target):\n",
    "    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "    rmse_scores  = np.sqrt(-1 * neg_mse_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    print('##### ',model.__class__.__name__ , ' #####')\n",
    "    print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom xgboost import XGBRegressor\\nfrom lightgbm import LGBMRegressor\\n\\ndt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)\\nrf_reg = RandomForestRegressor(random_state=0, n_estimators=1000)\\ngb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)\\nxgb_reg = XGBRegressor(n_estimators=1000)\\nlgb_reg = LGBMRegressor(n_estimators=1000)\\n\\n# 트리 기반의 회귀 모델을 반복하면서 평가 수행 \\nmodels = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg]\\nfor model in models:  \\n    get_model_cv_prediction(model, X_data, y_target)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)\n",
    "rf_reg = RandomForestRegressor(random_state=0, n_estimators=1000)\n",
    "gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)\n",
    "xgb_reg = XGBRegressor(n_estimators=1000)\n",
    "lgb_reg = LGBMRegressor(n_estimators=1000)\n",
    "\n",
    "# 트리 기반의 회귀 모델을 반복하면서 평가 수행 \n",
    "models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg]\n",
    "for model in models:  \n",
    "    get_model_cv_prediction(model, X_data, y_target)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import seaborn as sns\\n%matplotlib inline\\n\\nrf_reg = RandomForestRegressor(n_estimators=1000)\\n\\n# 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다.   \\nrf_reg.fit(X_data, y_target)\\n\\nfeature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns )\\nfeature_series = feature_series.sort_values(ascending=False)\\nsns.barplot(x= feature_series, y=feature_series.index)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=1000)\n",
    "\n",
    "# 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다.   \n",
    "rf_reg.fit(X_data, y_target)\n",
    "\n",
    "feature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns )\n",
    "feature_series = feature_series.sort_values(ascending=False)\n",
    "sns.barplot(x= feature_series, y=feature_series.index)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nbostonDF_sample = bostonDF[[\\'RM\\',\\'PRICE\\']]\\nbostonDF_sample = bostonDF_sample.sample(n=100,random_state=0)\\nprint(bostonDF_sample.shape)\\nplt.figure()\\nplt.scatter(bostonDF_sample.RM , bostonDF_sample.PRICE,c=\"darkorange\")'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "bostonDF_sample = bostonDF[['RM','PRICE']]\n",
    "bostonDF_sample = bostonDF_sample.sample(n=100,random_state=0)\n",
    "print(bostonDF_sample.shape)\n",
    "plt.figure()\n",
    "plt.scatter(bostonDF_sample.RM , bostonDF_sample.PRICE,c=\"darkorange\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# 선형 회귀와 결정 트리 기반의 Regressor 생성. DecisionTreeRegressor의 max_depth는 각각 2, 7\\nlr_reg = LinearRegression()\\nrf_reg2 = DecisionTreeRegressor(max_depth=2)\\nrf_reg7 = DecisionTreeRegressor(max_depth=7)\\n\\n# 실제 예측을 적용할 테스트용 데이터 셋을 4.5 ~ 8.5 까지 100개 데이터 셋 생성. \\nX_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1)\\n\\n# 보스턴 주택가격 데이터에서 시각화를 위해 피처는 RM만, 그리고 결정 데이터인 PRICE 추출\\nX_feature = bostonDF_sample['RM'].values.reshape(-1,1)\\ny_target = bostonDF_sample['PRICE'].values.reshape(-1,1)\\n\\n# 학습과 예측 수행. \\nlr_reg.fit(X_feature, y_target)\\nrf_reg2.fit(X_feature, y_target)\\nrf_reg7.fit(X_feature, y_target)\\n\\npred_lr = lr_reg.predict(X_test)\\npred_rf2 = rf_reg2.predict(X_test)\\npred_rf7 = rf_reg7.predict(X_test)\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 선형 회귀와 결정 트리 기반의 Regressor 생성. DecisionTreeRegressor의 max_depth는 각각 2, 7\n",
    "lr_reg = LinearRegression()\n",
    "rf_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "rf_reg7 = DecisionTreeRegressor(max_depth=7)\n",
    "\n",
    "# 실제 예측을 적용할 테스트용 데이터 셋을 4.5 ~ 8.5 까지 100개 데이터 셋 생성. \n",
    "X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1)\n",
    "\n",
    "# 보스턴 주택가격 데이터에서 시각화를 위해 피처는 RM만, 그리고 결정 데이터인 PRICE 추출\n",
    "X_feature = bostonDF_sample['RM'].values.reshape(-1,1)\n",
    "y_target = bostonDF_sample['PRICE'].values.reshape(-1,1)\n",
    "\n",
    "# 학습과 예측 수행. \n",
    "lr_reg.fit(X_feature, y_target)\n",
    "rf_reg2.fit(X_feature, y_target)\n",
    "rf_reg7.fit(X_feature, y_target)\n",
    "\n",
    "pred_lr = lr_reg.predict(X_test)\n",
    "pred_rf2 = rf_reg2.predict(X_test)\n",
    "pred_rf7 = rf_reg7.predict(X_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fig , (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3)\\n\\n# X축값을 4.5 ~ 8.5로 변환하며 입력했을 때, 선형 회귀와 결정 트리 회귀 예측 선 시각화\\n# 선형 회귀로 학습된 모델 회귀 예측선 \\nax1.set_title(\\'Linear Regression\\')\\nax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=\"darkorange\")\\nax1.plot(X_test, pred_lr,label=\"linear\", linewidth=2 )\\n\\n# DecisionTreeRegressor의 max_depth를 2로 했을 때 회귀 예측선 \\nax2.set_title(\\'Decision Tree Regression: \\n max_depth=2\\')\\nax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=\"darkorange\")\\nax2.plot(X_test, pred_rf2, label=\"max_depth:3\", linewidth=2 )\\n\\n# DecisionTreeRegressor의 max_depth를 7로 했을 때 회귀 예측선 \\nax3.set_title(\\'Decision Tree Regression: \\n max_depth=7\\')\\nax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=\"darkorange\")\\nax3.plot(X_test, pred_rf7, label=\"max_depth:7\", linewidth=2)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''fig , (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3)\n",
    "\n",
    "# X축값을 4.5 ~ 8.5로 변환하며 입력했을 때, 선형 회귀와 결정 트리 회귀 예측 선 시각화\n",
    "# 선형 회귀로 학습된 모델 회귀 예측선 \n",
    "ax1.set_title('Linear Regression')\n",
    "ax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=\"darkorange\")\n",
    "ax1.plot(X_test, pred_lr,label=\"linear\", linewidth=2 )\n",
    "\n",
    "# DecisionTreeRegressor의 max_depth를 2로 했을 때 회귀 예측선 \n",
    "ax2.set_title('Decision Tree Regression: \\n max_depth=2')\n",
    "ax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=\"darkorange\")\n",
    "ax2.plot(X_test, pred_rf2, label=\"max_depth:3\", linewidth=2 )\n",
    "\n",
    "# DecisionTreeRegressor의 max_depth를 7로 했을 때 회귀 예측선 \n",
    "ax3.set_title('Decision Tree Regression: \\n max_depth=7')\n",
    "ax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=\"darkorange\")\n",
    "ax3.plot(X_test, pred_rf7, label=\"max_depth:7\", linewidth=2)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
